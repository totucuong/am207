{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\Ex}{\\mathbb{E}}\n",
    "\\newcommand{\\Var}{\\mathrm{Var}}\n",
    "\\newcommand{\\Cov}{\\mathrm{Cov}}\n",
    "\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n",
    "\\newcommand{\\indic}{\\mathbb{1}}\n",
    "\\newcommand{\\avg}{\\overline}\n",
    "\\newcommand{\\est}{\\hat}\n",
    "\\newcommand{\\trueval}[1]{#1^{*}}\n",
    "\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\like}{\\cal L}\n",
    "\\renewcommand{\\loglike}{\\ell}\n",
    "\\renewcommand{\\err}{\\cal E}\n",
    "\\renewcommand{\\dat}{\\cal D}\n",
    "\\renewcommand{\\hyp}{\\cal H}\n",
    "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n",
    "\\renewcommand{\\x}{\\mathbf x}\n",
    "\\renewcommand{\\v}[1]{\\mathbf #1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we will learn about Markov Chain and its application in Monte Carlo Markov Chain. \n",
    "\n",
    "- Markov Chain is a model that describe a sequence of random variable $X_0, X_1, \\ldots, X_n, \\ldots$. They are all drawn from a distribution, where the value of $X_n$ depends only on the immediate variable $X_{n-1}$. In other words, it lies happily between **complete independence assumption** and **complete  dependence assumption**.\n",
    "\n",
    "- Monte Carlo Markov Chain is a simulation method (Monte Carlo) that generate samples from a distribution $f(x)$ indirectly using a Markov Chain. In this notebook, we will learn about two pouplar MCMC algorithms: **Metropolis-Hastings algorithm** and **Gibbs sampling**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain (MC)\n",
    "\n",
    "Markov chain lives in both in space and time. Space here means the domain of $X_t$, and time here means the domain of $t$. Both space and time can be discrete or continuous. We will first focus only on discrete domains. Moreover our discrete space will be finite also. That is $dom(X_t) = \\{1,2,\\ldots, M\\}$\n",
    "\n",
    "We now define formally Markov chains:\n",
    "\n",
    "   A sequence of random variables $X_0, X_1, \\ldots, X_n, \\ldots$ taking values in the state space $\\{1,2,\\ldots, M\\}$ is called a **Markov chain** if for all $n \\geq 0$, $$p(X_{n+1} = j \\mid X_n = i, X_{n-1} = i_{n-1}, \\ldots, X_0 = i_0) = p(X_{n+1} = j \\mid X_{n} = i)$$\n",
    "   \n",
    "The quantity $p(X_{n+1} = j \\mid X_{n} = i)$ is called the transition probability from state i to state j.\n",
    "\n",
    "**Time-homogenenous** Markov chain has the transition probability $p(X_{n+1} = j \\mid X_{n} = i)$ unchanged in time.\n",
    "\n",
    "**Markov property** is the condition in the defnition. It says that we need only $X_n$ to predict $X_{n+1}$. If n is the presents, before n is the past, after n is the future, Markov property says that given the present the past and the future are conditionally independent.\n",
    "\n",
    "Knowing the dynamics of a Markov chain means knowing its transition probability from one state to another state. These dynamics is encoded in a matrix, called the transiation matrix, whose (i,j) entry is the probability going from state i to state j in one step of the chain: M-by-M matrix $Q = (q_{ij})$\n",
    "\n",
    "An example of Markov chain: \n",
    "\n",
    "![example](gfx/mchain.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-step transition probability** from i to j is the probability of being at j exactly n steps after **staring** at i. We denote this by $q_{ij}^{(n)} = p(X_n = j \\mid X_0 = i)$.\n",
    "\n",
    "Note that $q_{ij}^{(2)} = \\sum_{k}q_{ik}q_{kj}$\n",
    "\n",
    "The right hand side is the (ij) entry of $Q^2$. We can say that $Q^2$ is the two-step transition probabilities. By induction, the nth power. In other words, the ith row of $Q^n$ is the conditional PMF of $X_n$ given $X_0 = i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mariginal distributions** $p(X_1), \\ldots, p(X_n), \\ldots$. To compute these distributions, we need $Q$ and the intial distribution of $X_0$. Suppose the marginal distribution $p(X_0) = (t_1, \\dots, t_M)$, then the marginal distribution at any time can be computed from the transition matrix, averaging over all the state using LOTP. \n",
    "\n",
    "Define $\\v{t} = (t_1, \\dots, t_M)$ by $t_i = p(X_0 = i)$. Then the marginal distribution of $X_n$ is given by the vector $\\v{t}Q^n$, viewing $\\v{t}$ as row vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different charateristics of a Markov chain\n",
    "\n",
    "We now study a number of characteristics of a Markov chain. They are important in understanding the long-run behaviour of the Markov chain. \n",
    "\n",
    "**recurrent** versus **transient**\n",
    "\n",
    "    State i of a Markov chain is recurrent if starting from i, the probability is 1 that the chain will eventually return to i. Otherwise, the state is transient, which means that if the chain starts from i, there is a positive probability of never returning to i.\n",
    "\n",
    "Recall that the number of failures, in repeated Bernoulli trials, before the first successful trial follow the Geometric distribution with parameter p: $X \\sim Geom(p),$ where $p(X = k) = q^kp$. Note that in ``scipy.stats.geom`` have a slightly different definition, where k counts also the success trial.\n",
    "![recurrent_transient](gfx/recurrent.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import geom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mygeom(k, p):\n",
    "    return (1-p)**k*p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111f2b9e8>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f2b908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = np.arange(10) \n",
    "plt.plot(k, mygeom(k,1/2), 'o', color='r', alpha=0.3)\n",
    "plt.plot(k, geom.pmf(k+1, 1/2), 'x', color='b', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a connection between transient states and geometric distribution: **the number of returns to transient state is Geometric**. Let i be a transient state of a Markov chain. Suppose the probability of never returning to i, starting from i, is a positive number $p >0$. Then, starting from i, the number of times that the chain returns to i before leaving forever is distributed Geom(p)\n",
    "\n",
    "This means, the chain will eventually leave state i forever. I understand this as probability of the number of times the chain returns before leaving forever approaches 0 as this number increase (See the graph). Hence eventually, the chain leave i forever.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if a state is recurrent? We cannot analyze the graph of Markov chain visually if there are too many states. **Irreducibility** allows us to do that: if it is possible to get from any state to any other state.\n",
    "\n",
    "**Irreducible and reducible chain**. \n",
    "        \n",
    "        A Markov chain with transition matrix $Q$ is irreducible if for any two states i and j, it is possible to go from i to j in a finite number of steps (with positive probability). That is, for any states i, j there is some positive integer n such that the (i,j) entry of $Q^n$ is positive. A Markov chain that is not irreducible is called reducible.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to prove irreducibility implies all states are reccurrent.\n",
    "\n",
    "We have irreducbile implies all recurrent. But the converse is not true. We can have a reducible Markov chain whose states are all recurrent. For example:\n",
    "\n",
    "![recurrent reducible](gfx/reducible_recurrent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**period of a state, periodic and aperiodic chain**\n",
    "\n",
    "The period of a state i in a Markov chain is the greated common divisor (gcd) of a the possible numbers of steps it can take to return to i when starting at i. In other words, the period of i is the gcd of numbers n such that the (i,i) entry of $Q^n$ is positive.\n",
    "\n",
    "A state is called **aperiodic** if its period equals 1, and period otherwise. The chain itself is called aperiodic if all its states are aperiodic, and periodic otherwise\n",
    "\n",
    "\n",
    "![aperiodic](gfx/aperiodic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary distribution\n",
    "\n",
    "We are interested in the behavior of Markov chain in long run. At first, the chain may spend tim in transient states. Eventually though, the chain will spend all its time in recurrent states. But what fraction of time will it spend in each of the recurrent states? This is answered by the **stationary distribution** of the chain, also known as the **steady-state** distribution.\n",
    "\n",
    "We will show that for irreducible and aperiodic Markov chains, the stationary distribution describles the long-run behavior of the chain, regardless of its intial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stationary distribution**. A row vector $\\v{s} = (s_1, \\ldots, s_M)$ such that $s_i \\geq 0$ and $\\sum_is_i = 1$ is a stationary distribution for a Markov chain with transition matrix Q if $$\\sum_is_iq_{ij} = s_j$$ for all j, ore equivalently, $$\\v{s}Q = \\v{s}$$\n",
    "\n",
    "From the definition, we can see that if the distribution of intial state $X_0$ is $\\v{s}$, then the distribution of $X_1$, $X_2$, etc., all will have distribution $\\v{s}$. In other words, if a Markov chain starts with its stationary distribution, it will stay in it forever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existence and uniqueness\n",
    "\n",
    "For finite state space, a stationary distribution always exists (This is due to Perron-Frobenious theorem)\n",
    "\n",
    "Any irreducible Markov chain has an unique stationary distribution. In this distribution, every state has positive probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "\n",
    "We have the following important theorem without proof.\n",
    "\n",
    "**Convergence to stationary distribution**.  Let $X_0, X_1, \\ldots$ be a Markov chain with stationary distribution $\\v{s}$ and transition matrix $Q$, such that some power $Q^m$ is positive in all entries. (These assumptions are equivalent to assuming that the chain is irreducible and aperiodic.) Then $P(X_n = i) \\rightarrow s_i$ as $n \\rightarrow \\infty$. In terms of the transition matrix, $Q^n$ converges to a matrix in which each row is s.\n",
    "Therefore, after a large number of steps, the probability that the chain is in state i is close to the stationary probability si, regardless of the chain's initial conditions. Intuitively, the extra condition of aperiodicity is needed in order to rule out chains that just go around in circles, such as the chain in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import matrix_power\n",
    "Q = np.array([[0,1,0,0,0], [0, 0, 1, 0, 0], [0,0,0,1,0], [0,0,0,0,1], [1,0,0,0,0]])\n",
    "intial = np.array([1,0,0,0,0])\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  10\n",
      "[1 0 0 0 0]\n",
      "------------------------------\n",
      "n =  11\n",
      "[0 1 0 0 0]\n",
      "------------------------------\n",
      "n =  12\n",
      "[0 0 1 0 0]\n",
      "------------------------------\n",
      "n =  13\n",
      "[0 0 0 1 0]\n",
      "------------------------------\n",
      "n =  14\n",
      "[0 0 0 0 1]\n",
      "------------------------------\n",
      "n =  15\n",
      "[1 0 0 0 0]\n",
      "------------------------------\n",
      "n =  16\n",
      "[0 1 0 0 0]\n",
      "------------------------------\n",
      "n =  17\n",
      "[0 0 1 0 0]\n",
      "------------------------------\n",
      "n =  18\n",
      "[0 0 0 1 0]\n",
      "------------------------------\n",
      "n =  19\n",
      "[0 0 0 0 1]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n in range(10,20):\n",
    "    print('n = ', n) \n",
    "    print(np.dot(intial.T, matrix_power(Q,n)))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.66666667],\n",
       "       [0.5       , 0.5       ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transition\n",
    "import numpy as np\n",
    "transition_matrix = np.array([[1/3, 2/3], [0.5, 0.5]])\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44444444 0.55555556]\n",
      " [0.41666667 0.58333333]]\n",
      "-----------------\n",
      "[[0.42592593 0.57407407]\n",
      " [0.43055556 0.56944444]]\n",
      "-----------------\n",
      "[[0.42901235 0.57098765]\n",
      " [0.42824074 0.57175926]]\n",
      "-----------------\n",
      "[[0.42849794 0.57150206]\n",
      " [0.42862654 0.57137346]]\n",
      "-----------------\n",
      "[[0.42858368 0.57141632]\n",
      " [0.42856224 0.57143776]]\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# now for higher n. We should see that it converges to the\n",
    "# stationary distribution\n",
    "tm_before = transition_matrix\n",
    "for i in range(5):\n",
    "    tm_next = np.dot(tm_before, transition_matrix)\n",
    "    print(tm_next)\n",
    "    print(\"-----------------\")\n",
    "    tm_before = tm_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the stationary distribution analytically. Assume that it is  $s = [p,1-p]$. Then $$sT = s$$ give us $$p(1/3) + (1-p)(1/2) = p$$\n",
    "\n",
    "and thus $p = 3/7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that we can get to this stationary distribution starting from multiple places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reversibility \n",
    "\n",
    "Stationary distribution is important to understand a Markov chain's long-run behavior. \n",
    "\n",
    "To find stationary distribution, we need to find eigenvectors of the transition matrix. This involving solving a high-degree polynomial which can be difficult when the number of states (hence the size of Q) is high.\n",
    "\n",
    "Fortunately, there is an important special cases where working with eginevalue equations for large matrices can be avoided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reversibiliy**. Let $Q = (q_{ij})$ be the transition matrix of a Markov chain. Suppose there is $\\v{s} = (s_1, \\dots, s_M)$ with $s_i \\geq 0$, $\\sum_i s_i = 1$, such that $$s_iq_{ij} = s_{j}q_{ji}$$\n",
    "\n",
    "for all states i and j.\n",
    "\n",
    "This equation is called the **reversibility** or **detailed balance** condition, and we say that the chain is **reversible** with respect to $\\v{s}$ if it holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a transition matrix, if we can find $s$ whose components sum to $1$ and which satisifies the reversibility condition, then $\\v{s}$ is automatically a stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain in continous state spaces\n",
    "\n",
    "In continous state spaces, the transition matrix T becomes an integral kernel K"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
