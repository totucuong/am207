{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\Ex}{\\mathbb{E}}\n",
    "\\newcommand{\\Var}{\\mathrm{Var}}\n",
    "\\newcommand{\\Cov}{\\mathrm{Cov}}\n",
    "\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n",
    "\\newcommand{\\indic}{\\mathbb{1}}\n",
    "\\newcommand{\\avg}{\\overline}\n",
    "\\newcommand{\\est}{\\hat}\n",
    "\\newcommand{\\trueval}[1]{#1^{*}}\n",
    "\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\like}{\\cal L}\n",
    "\\renewcommand{\\loglike}{\\ell}\n",
    "\\renewcommand{\\err}{\\cal E}\n",
    "\\renewcommand{\\dat}{\\cal D}\n",
    "\\renewcommand{\\hyp}{\\cal H}\n",
    "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n",
    "\\renewcommand{\\x}{\\mathbf x}\n",
    "\\renewcommand{\\v}[1]{\\mathbf #1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that data is generated by a distribution parameterized by a true parameter $\\theta^*$. Our job is to infer this value from the provided data. \n",
    "\n",
    "This is the parametric way of doing data analysis.\n",
    "\n",
    "The frequentist view is that $\\theta^*$ is fixed and we approximate it as $\\hat{\\theta}$ using our limited sample.\n",
    "\n",
    "A distribution is induced on this estimate by considering may samples that could have been drawn from the population. This distribution is called the **sampling distiribution of the parameter $\\theta$.\n",
    "\n",
    "How do we estimate $\\hat{\\theta}$? And how do we compute this sampling distribution so that we can have a notion of uncertainty of our estimation.\n",
    "\n",
    "To estimate we use the Maximum Likelihood estimate.\n",
    "\n",
    "To compute sampling distribution, we us bootstrap techniques.\n",
    "\n",
    "We will learn about MLE in the context of the exponetial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The idea behind the MLE\n",
    "\n",
    "![mle](gfx/gaussmle.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, we have two distributions from the same family but with different parameters, $\\theta = 1.8$ and $\\theta = 5.8$. And suppose we have 3 data points, the red points.\n",
    "\n",
    "MLE idea is given data, which ditribution is the data more likely to have come from?\n",
    "\n",
    "If we assume the data points are i.i.d then the probability of data will be the product of the probabiliy of each data points. In the above picture, this data likelihood will be the product of three vertical length. Obviously the blue bars will have bigger product than that of the green ones.\n",
    "\n",
    "So the question turns into how do we move around the distribution using $\\theta$ so that this product is maximized.\n",
    "\n",
    "We have the likelihood of data $$L(\\lambda) = \\prod_{i=1}^{n}p(x_i|\\lambda)$$\n",
    "\n",
    "Often it is easier and numerically more stable to maximise the log likelihood: $$l(\\lambda) = \\sum_{i=1}^{n}\\ln(p(x_i|\\lambda)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we assume that the distribution is an exponential distribution $f(x,\\lambda) = \\lambda e^{-\\lambda x}$. Then maximize the log likelihood will be achieved by setting $\\frac{1}{\\lambda} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$\n",
    "\n",
    "In the case of exponetial distribution, we were able to maximize the log likelihood analytically, i.e., having a formula. Normally we would have to use numerical optimization procedure such as gradient descent. \n",
    "\n",
    "A crucial property is that, for many commonly occurring situations, maximum likelihood parameter estimators have an approximate normal distribution when n is large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "We can use bootstrap as a way to build sampling distribution of our estimation $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](gfx/linregmle.png)\n",
    "\n",
    "The fundamental assumption for the probabilistic analysis of linear regression is that each $y_i$ is gaussian distributed with mean  $\\v{w}\\cdot\\v{x_i}$ (the y predicted by the regression line so to speak) and variance $\\sigma^2$:\n",
    "\n",
    "$$ y_i \\sim N(\\v{w}\\cdot \\v{x_i}, \\sigma^2) .$$\n",
    "\n",
    "We can then write the likelihood:\n",
    "\n",
    "$$\\cal{L} = p(\\v{y} | \\v{x}, \\v{w}, \\sigma) = \\prod_i p(\\v{y}_i | \\v{x}_i, \\v{w}, \\sigma)$$\n",
    "\n",
    "Given the canonical form of the gaussian:\n",
    "\n",
    "$$N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(y - \\mu)^2 / 2\\sigma^2},$$\n",
    "\n",
    "we can show that:\n",
    "\n",
    "$$\\cal{L} =  (2\\pi\\sigma^2)^{(-n/2)} e^{\\frac{-1}{2\\sigma^2} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2} .$$\n",
    "\n",
    "The log likelihood $\\ell$ then is given by:\n",
    "\n",
    "$$\\ell = \\frac{-n}{2} log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2 .$$\n",
    "\n",
    "If you differentiate this with respect to  $\\v{w}$ and $\\sigma$, you get the MLE values of the parameter estimates:\n",
    "\n",
    "$$\\v{w}_{MLE} = (\\v{X}^T\\v{X})^{-1} \\v{X}^T\\v{y}, $$\n",
    "\n",
    "where $\\v{X}$ is the design matrix created by stacking rows $\\v{x}_i$, and\n",
    "\n",
    "$$\\sigma^2_{MLE} =  \\frac{1}{n} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2  . $$\n",
    "\n",
    "These are the standard results of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression if one of the well known **supervized** learning algorithms used for classification.\n",
    "\n",
    "The idea behind logistic regression is very simple. We want to draw a line in feature space that divides the '1' samples from the '0' samples, just like in the diagram above. In other words, we wish to find the \"regression\" line which divides the samples. Now, a line has the form $w_1 x_1 + w_2 x_2 + w_0 = 0$ in 2-dimensions. On one side of this line we have \n",
    "\n",
    "$$w_1 x_1 + w_2 x_2 + w_0 \\ge 0,$$\n",
    "\n",
    "and on the other side we have \n",
    "\n",
    "$$w_1 x_1 + w_2 x_2 + w_0 < 0.$$ \n",
    "\n",
    "Our classification rule then becomes:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "y = 1 &if& \\v{w}\\cdot\\v{x} \\ge 0\\\\\n",
    "y = 0 &if& \\v{w}\\cdot\\v{x} < 0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $\\v{x}$ is the vector $\\{1,x_1, x_2,...,x_n\\}$ where we have also generalized to more than 2 features.\n",
    "\n",
    "What hypotheses $h$ can we use to achieve this? One way to do so is to use the **sigmoid** function:\n",
    "\n",
    "$$h(z) = \\frac{1}{1 + e^{-z}}.$$\n",
    "\n",
    "Notice that at $z=0$ this function has the value 0.5. If $z > 0$, $h > 0.5$ and as $z \\to \\infty$, $h \\to 1$. If $z < 0$, $h < 0.5$ and as $z \\to -\\infty$, $h \\to 0$. As long as we identify any value of $y > 0.5$ as 1, and any $y < 0.5$ as 0, we can achieve what we wished above.\n",
    "\n",
    "This function is plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c16ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "h = lambda z: 1./(1+np.exp(-z))\n",
    "zs=np.arange(-5,5,0.1)\n",
    "plt.plot(zs, h(zs), alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said above that if $h > 0.5$ we ought to identify the sample with $y=1$? One way of thinking about this is to identify $h(\\v{w}\\cdot\\v{x})$ with the probability that the sample is a '1' ($y=1$). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a '1' is $\\ge 0.5$.\n",
    "\n",
    "So suppose we say then that the probability of $y=1$ for a given $\\v{x}$ is given by $h(\\v{w}\\cdot\\v{x})$?\n",
    "\n",
    "Then, the conditional probabilities of $y=1$ or $y=0$ given a particular sample's features $\\v{x}$ are:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "P(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\n",
    "P(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "These two can be written together as\n",
    "\n",
    "$$P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} $$\n",
    "\n",
    "Then multiplying over the samples we get the probability of the training $y$ given $\\v{w}$ and the $\\v{x}$:\n",
    "\n",
    "$$P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}$$\n",
    "\n",
    "Why use probabilities? Earlier, we talked about how the regression function $f(x)$ never gives us the $y$ exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently. \n",
    "\n",
    "We said that another way to think about a noisy $y$ is to imagine that our data $\\dat$ was generated from  a joint probability distribution $P(x,y)$. Thus we need to model $y$ at a given $x$, written as $P(y \\mid x)$, and since $P(x)$ is also a probability distribution, we have:\n",
    "\n",
    "$$P(x,y) = P(y \\mid x) P(x) ,$$\n",
    "\n",
    "and can obtain our joint probability ($P(x, y))$.\n",
    "\n",
    "Indeed its important to realize that a particular sample can be thought of as a draw from some \"true\" probability distribution. If for example the probability of classifying a sample point as a '0' was 0.1, and it turns out that the sample point was actually a '0', it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a '0'! But, of-course its more unlikely than its likely, and having good probabilities means that we'll be likely right most of the time, which is what we want to achieve in classification. \n",
    "\n",
    "Thus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a '1'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we maximize $$P(y \\mid \\v{x},\\v{w})$$, we will maximize the chance that each point is classified correctly, which is what we want to do. This is a principled way of obtaining the highest probability classification. This **maximum likelihood** estimation maximises the **likelihood of the sample y**, \n",
    "\n",
    "$$\\like = P(y \\mid \\v{x},\\v{w}).$$ \n",
    "\n",
    "\n",
    "Again, we can equivalently maximize \n",
    "\n",
    "$$\\loglike = log(P(y \\mid \\v{x},\\v{w}))$$ \n",
    "\n",
    "since the natural logarithm $log$ is a monotonic function. This is known as maximizing the **log-likelihood**.\n",
    "\n",
    "\n",
    "$$\\loglike = log \\like = log(P(y \\mid \\v{x},\\v{w})).$$\n",
    "\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\loglike &=& log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n",
    "                  &=& \\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n",
    "                  &=& \\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n",
    "                  &=& \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
