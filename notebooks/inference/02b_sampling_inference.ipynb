{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP (Most probably merge with 02_approximate_inference\n",
    "# Computing on Bayes Network\n",
    "To understand Pyro's behaviour, it is critical to understand the basic of how to approximate different probability queries using sampling on Bayes networks. Let's take the classical Student network from Koller's book:\n",
    "\n",
    "![Student network](../gfx/student_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the probability of an event Y=y, P(Y) =?\n",
    "\n",
    "This is easy. Because can perform topological ordering of the network's node. Then perform **forward-sampling** (ancestral sampling) on networks node following the topological ordering. We have the estimation of $P(Y=y)$ by generating M samples and: \n",
    "\n",
    "$$P(Y=y) = \\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{1}(y[m] = y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing conditional probability of an event P(Y=y|E=e)?\n",
    "\n",
    "This is much harder than the unconditional version. The most obvious solution is to generate M samples and keep only those samples that are consistent with E=e. Then we we can estimate P(Y=y|E=e) as in the unconditional one. This is called **Rejection sampling**. Unfortunately this method wastes a lot of computation. Because, in general, P(E=e) is normally very low so we need to generate a lot of samples (M>>0). \n",
    "\n",
    "## Likelihood-weighted particle generation\n",
    "This method uses forward-sampling to generate samples but with one modification. Since we have the observed variables (E=e), during the forward sampling, the observed values are assigned to observed variables, then the sampling is carried in downstream fashion as before. \n",
    "\n",
    "The key thing here is to reweight each samples according the the probaiblity of desired values in the observation E:\n",
    "\n",
    "$$w = \\prod_{i=1}^{|E|} p(e_i| pa_{e_i}),$$\n",
    "\n",
    "where $pa_{e_i}$ is the assignments to the parent nodes of $e_i$. Using these M weighted samples, we can estimate the conditional distribution as follows.\n",
    "\n",
    "$$P(y|e) = \\frac{\\sum_{m=1}^{M}w[m]\\mathbb{1}(y[m] = y)}{\\sum_{m=1}^{M}w[m]}$$\n",
    "\n",
    "![](../gfx/likehood_weighted_particle_generation.png)\n",
    "\n",
    "This likelihood-weighted method is a special case of importance sampling, which we study in the next section.\n",
    "\n",
    "## Importance sampling\n",
    "Importance sampling is normally used to estimate some expection of a function $f(X)$ wrt some distribution $p(X)$. But we can also use it to compute the probability of some event A, $P(A)$. Recall that $E[{\\mathbb{I}(A=a)}] =p(A=a)$. \n",
    "\n",
    "If we can generate M samples from $p(X)$ easily, then we can estimate this expection $E_{p}[f] = \\frac{1}{M}\\sum_{m=1}^{M}f(x[m])$. However it is not always the case. P(x) maybe difficult to sample from or it is not normalized. \n",
    "\n",
    "The key idea is to generate samples from another distribution $q$, which is assumed to be easily to be sampled from. And reweight the samples when estimate the expection. Depending on how we perform the reweighting, we have different importance sampling method.\n",
    "\n",
    "**Unnormalized importance sampling**: $$E[f] = \\frac{1}{M}\\sum_{m=1}^{M}f(x[m])\\frac{P(x)}{Q(x)}$$, It is called **unnormalized** because P is a normalized distribution, i.e., there is no need to perform normalization.\n",
    "\n",
    "**Normalized importance sampling**: In this case, we know $p(X)$ only up to a normalization factor, i.e., it is not normalized. Instead we have access only to its unormalized version $\\widetilde{p}(X)$. Define the weight of a sample to be $$w(X) = \\frac{\\widetilde{p}(X)}{q(X)}$$. Then the expection is estimated as follows. $$E[f] = \\frac{\\sum_{m=1}^{M}f(x[m])w(x[m])}{\\sum_{m=1}{w(x[m])}}$$\n",
    "\n",
    "Normalized importance sampling is equivalent to likelihood-weighted particle generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
