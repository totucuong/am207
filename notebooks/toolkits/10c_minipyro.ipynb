{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a notebook to explain how pyro works using their provided ``minipyro`` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dists\n",
    "from collections import OrderedDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pyro, we can build generative models such as this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    loc = param('loc', init_value=torch.tensor(0.2))\n",
    "    mean = sample('mean_rv', dists.Normal(loc=loc, scale=2.))\n",
    "    x = sample('x_rv', dists.Normal(loc=mean, scale=1.))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main primitives ``param`` and ``sample``. ``param`` is used to specify model parameters.  ``sample`` is used to construct random variables. Together with ``torch.layers`` we can build complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYRO_STACK = []\n",
    "PARAM_STORE = {}\n",
    "\n",
    "# sample is an effectful version of Distribution.sample(...)\n",
    "# When any effect handlers are active, it constructs an initial message and calls apply_stack.\n",
    "def sample(name, fn, obs=None):\n",
    "    # if there are no active Messengers, we just draw a sample and return it as expected:\n",
    "    if not PYRO_STACK:\n",
    "        return fn()\n",
    "\n",
    "    # Otherwise, we initialize a message...\n",
    "    initial_msg = {\n",
    "        \"type\": \"sample\",\n",
    "        \"name\": name,\n",
    "        \"fn\": fn,\n",
    "        \"args\": (),\n",
    "        \"value\": obs,\n",
    "    }\n",
    "\n",
    "    # ...and use apply_stack to send it to the Messengers\n",
    "    msg = apply_stack(initial_msg)\n",
    "    return msg[\"value\"]\n",
    "\n",
    "# param is an effectful version of PARAM_STORE.setdefault\n",
    "# When any effect handlers are active, it constructs an initial message and calls apply_stack.\n",
    "def param(name, init_value=None):\n",
    "\n",
    "    def fn(init_value):\n",
    "        value = PARAM_STORE.setdefault(name, init_value)\n",
    "        value.requires_grad_()\n",
    "        return value\n",
    "\n",
    "    # if there are no active Messengers, we just draw a sample and return it as expected:\n",
    "    if not PYRO_STACK:\n",
    "        return fn(init_value)\n",
    "\n",
    "    # Otherwise, we initialize a message...\n",
    "    initial_msg = {\n",
    "        \"type\": \"param\",\n",
    "        \"name\": name,\n",
    "        \"fn\": fn,\n",
    "        \"args\": (init_value,),\n",
    "        \"value\": None,\n",
    "    }\n",
    "\n",
    "    # ...and use apply_stack to send it to the Messengers\n",
    "    msg = apply_stack(initial_msg)\n",
    "    return msg[\"value\"]\n",
    "\n",
    "# apply_stack is called by pyro.sample and pyro.param.\n",
    "# It is responsible for applying each Messenger to each effectful operation.\n",
    "def apply_stack(msg):\n",
    "    for pointer, handler in enumerate(reversed(PYRO_STACK)): # execute handler from bottom\n",
    "        handler.process_message(msg)\n",
    "        # When a Messenger sets the \"stop\" field of a message,\n",
    "        # it prevents any Messengers above it on the stack from being applied.\n",
    "        if msg.get(\"stop\"):\n",
    "            break\n",
    "    if msg[\"value\"] is None:\n",
    "        msg[\"value\"] = msg[\"fn\"](*msg[\"args\"])\n",
    "\n",
    "    # A Messenger that sets msg[\"stop\"] == True also prevents application\n",
    "    # of postprocess_message by Messengers above it on the stack\n",
    "    # via the pointer variable from the process_message loop\n",
    "    for handler in PYRO_STACK[-pointer-1:]:\n",
    "        handler.postprocess_message(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the model to generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1805, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train or learn model parameters given observations. One way to do this is to build up a hierarchy of event handlers or Messengers. They can be used to manipulate probablistic program such as model() above. We now showcase how we build a ``trace`` messenger which basically records the sampling process of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base effect handler class (called Messenger here for consistency with Pyro).\n",
    "class Messenger(object):\n",
    "    def __init__(self, fn=None):\n",
    "        self.fn = fn\n",
    "\n",
    "    # Effect handlers push themselves onto the PYRO_STACK.\n",
    "    # Handlers earlier in the PYRO_STACK are applied first.\n",
    "    def __enter__(self):\n",
    "        PYRO_STACK.append(self)\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        assert PYRO_STACK[-1] is self\n",
    "        PYRO_STACK.pop()\n",
    "\n",
    "    def process_message(self, msg):\n",
    "        pass\n",
    "\n",
    "    def postprocess_message(self, msg):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with self:\n",
    "            return self.fn(*args, **kwargs)\n",
    "        \n",
    "# A first useful example of an effect handler.\n",
    "# trace records the inputs and outputs of any primitive site it encloses,\n",
    "# and returns a dictionary containing that data to the user.\n",
    "class trace(Messenger):\n",
    "    def __enter__(self):\n",
    "        super(trace, self).__enter__()\n",
    "        self.trace = OrderedDict()\n",
    "        return self.trace\n",
    "\n",
    "    # trace illustrates why we need postprocess_message in addition to process_message:\n",
    "    # We only want to record a value after all other effects have been applied\n",
    "    def postprocess_message(self, msg):\n",
    "        print(f'postprocessing message {msg}')\n",
    "        assert msg[\"name\"] not in self.trace, \"all sites must have unique names\"\n",
    "        self.trace[msg[\"name\"]] = msg.copy()\n",
    "\n",
    "    def get_trace(self, *args, **kwargs):\n",
    "        self(*args, **kwargs)\n",
    "        return self.trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postprocessing message {'type': 'param', 'name': 'loc', 'fn': <function param.<locals>.fn at 0x123e3bd08>, 'args': (tensor(0.2000),), 'value': tensor(0.2000, requires_grad=True)}\n",
      "postprocessing message {'type': 'sample', 'name': 'mean_rv', 'fn': Normal(loc: 0.20000000298023224, scale: 2.0), 'args': (), 'value': tensor(-1.1385, grad_fn=<AddBackward0>)}\n",
      "postprocessing message {'type': 'sample', 'name': 'x_rv', 'fn': Normal(loc: -1.1385059356689453, scale: 1.0), 'args': (), 'value': tensor(-1.0016, grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "with trace() as t:\n",
    "    model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this works is as follows:\n",
    "    1. sample or param if contained in a Messenger will create a msg call apply_stack(msg)\n",
    "    2. apply_stack go from bottom of PYRO_STACK and execute each messenger  process_message(msg). This means the original msg will get modified as it is modified by messengers from bottom to top of the stack.\n",
    "    3. then apply_stack will go from top to bottom again, but this time calling post_process_message(msg). The reason this step is necessary is demonstrated in trace Messenger. One example i can think of is, imagine we have a pyro stack as\n",
    "    [cond1, trace, cond2, cond3], (extreme left is top, extreme right is bottom). So as msg get passed from right to left cond3->cond2->trace->cond1 and trace contracts its trace only in process_message it may not record the changes cond1 makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical message for ``param`` primitives:\n",
    "\n",
    "```python\n",
    "           {'type': 'param',\n",
    "           'name': 'loc',\n",
    "           'fn': <function __main__.param.<locals>.fn(init_value)>,\n",
    "           'args': (tensor(0.2000),),\n",
    "           'value': tensor(0.2000, requires_grad=True)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical message for ``sample`` primitives:\n",
    "\n",
    "``` python\n",
    "               {'type': 'sample',\n",
    "               'name': 'mean_rv',\n",
    "               'fn': Normal(loc: 0.20000000298023224, scale: 2.0),\n",
    "               'args': (),\n",
    "               'value': tensor(3.7249, grad_fn=<AddBackward0>)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postprocessing message {'type': 'param', 'name': 'loc', 'fn': <function param.<locals>.fn at 0x124073620>, 'args': (tensor(0.2000),), 'value': tensor(0.2000, requires_grad=True)}\n",
      "postprocessing message {'type': 'sample', 'name': 'mean_rv', 'fn': Normal(loc: 0.20000000298023224, scale: 2.0), 'args': (), 'value': tensor(0.4489, grad_fn=<AddBackward0>)}\n",
      "postprocessing message {'type': 'sample', 'name': 'x_rv', 'fn': Normal(loc: 0.44888389110565186, scale: 1.0), 'args': (), 'value': tensor(-0.0070, grad_fn=<AddBackward0>)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('loc',\n",
       "              {'type': 'param',\n",
       "               'name': 'loc',\n",
       "               'fn': <function __main__.param.<locals>.fn(init_value)>,\n",
       "               'args': (tensor(0.2000),),\n",
       "               'value': tensor(0.2000, requires_grad=True)}),\n",
       "             ('mean_rv',\n",
       "              {'type': 'sample',\n",
       "               'name': 'mean_rv',\n",
       "               'fn': Normal(loc: 0.20000000298023224, scale: 2.0),\n",
       "               'args': (),\n",
       "               'value': tensor(0.4489, grad_fn=<AddBackward0>)}),\n",
       "             ('x_rv',\n",
       "              {'type': 'sample',\n",
       "               'name': 'x_rv',\n",
       "               'fn': Normal(loc: 0.44888389110565186, scale: 1.0),\n",
       "               'args': (),\n",
       "               'value': tensor(-0.0070, grad_fn=<AddBackward0>)})])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracer.get_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sth(*args, **kwargs):\n",
    "    print('arguments: ', args)\n",
    "    print('keyword arguments: ', kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside context:  [<__main__.Messenger object at 0x123dd6710>]\n",
      "outside context:  []\n"
     ]
    }
   ],
   "source": [
    "with Messenger('sample_sth'):\n",
    "    print('inside context: ', PYRO_STACK)\n",
    "print('outside context: ', PYRO_STACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "messenger = Messenger(sample_sth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments:  (23, 233)\n",
      "keyword arguments:  {'lr': 0.01, 'opt': 33}\n"
     ]
    }
   ],
   "source": [
    "messenger(23, 233, lr=0.01, opt=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mini Pyro\n",
    "---------\n",
    "\n",
    "This file contains a minimal implementation of the Pyro Probabilistic\n",
    "Programming Language. The API (method signatures, etc.) match that of\n",
    "the full implementation as closely as possible. This file is independent\n",
    "of the rest of Pyro, with the exception of the :mod:`pyro.distributions`\n",
    "module.\n",
    "\n",
    "An accompanying example that makes use of this implementation can be\n",
    "found at examples/minipyro.py.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "\n",
    "# Pyro keeps track of two kinds of global state:\n",
    "# i)  The effect handler stack, which enables non-standard interpretations of\n",
    "#     Pyro primitives like sample();\n",
    "#     See http://docs.pyro.ai/en/0.3.0-release/poutine.html\n",
    "# ii) Trainable parameters in the Pyro ParamStore;\n",
    "#     See http://docs.pyro.ai/en/0.3.0-release/parameters.html\n",
    "\n",
    "PYRO_STACK = []\n",
    "PARAM_STORE = {}\n",
    "\n",
    "\n",
    "def get_param_store():\n",
    "    return PARAM_STORE\n",
    "\n",
    "\n",
    "# The base effect handler class (called Messenger here for consistency with Pyro).\n",
    "class Messenger(object):\n",
    "    def __init__(self, fn=None):\n",
    "        self.fn = fn\n",
    "\n",
    "    # Effect handlers push themselves onto the PYRO_STACK.\n",
    "    # Handlers earlier in the PYRO_STACK are applied first.\n",
    "    def __enter__(self):\n",
    "        PYRO_STACK.append(self)\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        assert PYRO_STACK[-1] is self\n",
    "        PYRO_STACK.pop()\n",
    "\n",
    "    def process_message(self, msg):\n",
    "        pass\n",
    "\n",
    "    def postprocess_message(self, msg):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with self:\n",
    "            return self.fn(*args, **kwargs)\n",
    "\n",
    "\n",
    "# A first useful example of an effect handler.\n",
    "# trace records the inputs and outputs of any primitive site it encloses,\n",
    "# and returns a dictionary containing that data to the user.\n",
    "class trace(Messenger):\n",
    "    def __enter__(self):\n",
    "        super(trace, self).__enter__()\n",
    "        self.trace = OrderedDict()\n",
    "        return self.trace\n",
    "\n",
    "    # trace illustrates why we need postprocess_message in addition to process_message:\n",
    "    # We only want to record a value after all other effects have been applied\n",
    "    def postprocess_message(self, msg):\n",
    "        assert msg[\"name\"] not in self.trace, \"all sites must have unique names\"\n",
    "        self.trace[msg[\"name\"]] = msg.copy()\n",
    "\n",
    "    def get_trace(self, *args, **kwargs):\n",
    "        self(*args, **kwargs)\n",
    "        return self.trace\n",
    "\n",
    "\n",
    "# A second example of an effect handler for setting the value at a sample site.\n",
    "# This illustrates why effect handlers are a useful PPL implementation technique:\n",
    "# We can compose trace and replay to replace values but preserve distributions,\n",
    "# allowing us to compute the joint probability density of samples under a model.\n",
    "# See the definition of elbo(...) below for an example of this pattern.\n",
    "class replay(Messenger):\n",
    "    def __init__(self, fn, guide_trace):\n",
    "        self.guide_trace = guide_trace\n",
    "        super(replay, self).__init__(fn)\n",
    "\n",
    "    def process_message(self, msg):\n",
    "        if msg[\"name\"] in self.guide_trace:\n",
    "            msg[\"value\"] = self.guide_trace[msg[\"name\"]][\"value\"]\n",
    "\n",
    "\n",
    "# block allows the selective application of effect handlers to different parts of a model.\n",
    "# Sites hidden by block will only have the handlers below block on the PYRO_STACK applied,\n",
    "# allowing inference or other effectful computations to be nested inside models.\n",
    "class block(Messenger):\n",
    "    def __init__(self, fn=None, hide_fn=lambda msg: True):\n",
    "        self.hide_fn = hide_fn\n",
    "        super(block, self).__init__(fn)\n",
    "\n",
    "    def process_message(self, msg):\n",
    "        if self.hide_fn(msg):\n",
    "            msg[\"stop\"] = True\n",
    "\n",
    "\n",
    "# This limited implementation of PlateMessenger only implements broadcasting.\n",
    "class PlateMessenger(Messenger):\n",
    "    def __init__(self, fn, size, dim):\n",
    "        assert dim < 0\n",
    "        self.size = size\n",
    "        self.dim = dim\n",
    "        super(PlateMessenger, self).__init__(fn)\n",
    "\n",
    "    def process_message(self, msg):\n",
    "        if msg[\"type\"] == \"sample\":\n",
    "            batch_shape = msg[\"fn\"].batch_shape\n",
    "            if len(batch_shape) < -self.dim or batch_shape[self.dim] != self.size:\n",
    "                batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape)\n",
    "                batch_shape[self.dim] = self.size\n",
    "                msg[\"fn\"] = msg[\"fn\"].expand(torch.Size(batch_shape))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return range(self.size)\n",
    "\n",
    "\n",
    "# apply_stack is called by pyro.sample and pyro.param.\n",
    "# It is responsible for applying each Messenger to each effectful operation.\n",
    "def apply_stack(msg):\n",
    "    print('ok running apply stack')\n",
    "    for pointer, handler in enumerate(reversed(PYRO_STACK)):\n",
    "        handler.process_message(msg)\n",
    "        # When a Messenger sets the \"stop\" field of a message,\n",
    "        # it prevents any Messengers above it on the stack from being applied.\n",
    "        if msg.get(\"stop\"):\n",
    "            break\n",
    "    if msg[\"value\"] is None:\n",
    "        msg[\"value\"] = msg[\"fn\"](*msg[\"args\"])\n",
    "\n",
    "    # A Messenger that sets msg[\"stop\"] == True also prevents application\n",
    "    # of postprocess_message by Messengers above it on the stack\n",
    "    # via the pointer variable from the process_message loop\n",
    "    for handler in PYRO_STACK[-pointer-1:]:\n",
    "        handler.postprocess_message(msg)\n",
    "    return msg\n",
    "\n",
    "\n",
    "# sample is an effectful version of Distribution.sample(...)\n",
    "# When any effect handlers are active, it constructs an initial message and calls apply_stack.\n",
    "def sample(name, fn, obs=None):\n",
    "\n",
    "    # if there are no active Messengers, we just draw a sample and return it as expected:\n",
    "    if not PYRO_STACK:\n",
    "        return fn()\n",
    "\n",
    "    # Otherwise, we initialize a message...\n",
    "    initial_msg = {\n",
    "        \"type\": \"sample\",\n",
    "        \"name\": name,\n",
    "        \"fn\": fn,\n",
    "        \"args\": (),\n",
    "        \"value\": obs,\n",
    "    }\n",
    "\n",
    "    # ...and use apply_stack to send it to the Messengers\n",
    "    msg = apply_stack(initial_msg)\n",
    "    return msg[\"value\"]\n",
    "\n",
    "\n",
    "# param is an effectful version of PARAM_STORE.setdefault\n",
    "# When any effect handlers are active, it constructs an initial message and calls apply_stack.\n",
    "def param(name, init_value=None):\n",
    "\n",
    "    def fn(init_value):\n",
    "        value = PARAM_STORE.setdefault(name, init_value)\n",
    "        value.requires_grad_()\n",
    "        return value\n",
    "\n",
    "    # if there are no active Messengers, we just draw a sample and return it as expected:\n",
    "    if not PYRO_STACK:\n",
    "        return fn(init_value)\n",
    "\n",
    "    # Otherwise, we initialize a message...\n",
    "    initial_msg = {\n",
    "        \"type\": \"param\",\n",
    "        \"name\": name,\n",
    "        \"fn\": fn,\n",
    "        \"args\": (init_value,),\n",
    "        \"value\": None,\n",
    "    }\n",
    "\n",
    "    # ...and use apply_stack to send it to the Messengers\n",
    "    msg = apply_stack(initial_msg)\n",
    "    return msg[\"value\"]\n",
    "\n",
    "\n",
    "# boilerplate to match the syntax of actual pyro.plate:\n",
    "def plate(name, size, dim):\n",
    "    return PlateMessenger(fn=None, size=size, dim=dim)\n",
    "\n",
    "\n",
    "# This is a thin wrapper around the `torch.optim.Adam` class that\n",
    "# dynamically generates optimizers for dynamically generated parameters.\n",
    "# See http://docs.pyro.ai/en/0.3.0-release/optimization.html\n",
    "class Adam(object):\n",
    "    def __init__(self, optim_args):\n",
    "        self.optim_args = optim_args\n",
    "        # Each parameter will get its own optimizer, which we keep track\n",
    "        # of using this dictionary keyed on parameters.\n",
    "        self.optim_objs = {}\n",
    "\n",
    "    def __call__(self, params):\n",
    "        for param in params:\n",
    "            # If we've seen this parameter before, use the previously\n",
    "            # constructed optimizer.\n",
    "            if param in self.optim_objs:\n",
    "                optim = self.optim_objs[param]\n",
    "            # If we've never seen this parameter before, construct\n",
    "            # an Adam optimizer and keep track of it.\n",
    "            else:\n",
    "                optim = torch.optim.Adam([param], **self.optim_args)\n",
    "                self.optim_objs[param] = optim\n",
    "            # Take a gradient step for the parameter param.\n",
    "            optim.step()\n",
    "\n",
    "\n",
    "# This is a unified interface for stochastic variational inference in Pyro.\n",
    "# The actual construction of the loss is taken care of by `loss`.\n",
    "# See http://docs.pyro.ai/en/0.3.0-release/inference_algos.html\n",
    "class SVI(object):\n",
    "    def __init__(self, model, guide, optim, loss):\n",
    "        self.model = model\n",
    "        self.guide = guide\n",
    "        self.optim = optim\n",
    "        self.loss = loss\n",
    "\n",
    "    # This method handles running the model and guide, constructing the loss\n",
    "    # function, and taking a gradient step.\n",
    "    def step(self, *args, **kwargs):\n",
    "        # This wraps both the call to `model` and `guide` in a `trace` so that\n",
    "        # we can record all the parameters that are encountered. Note that\n",
    "        # further tracing occurs inside of `loss`.\n",
    "        with trace() as param_capture:\n",
    "            # We use block here to allow tracing to record parameters only.\n",
    "            with block(hide_fn=lambda msg: msg[\"type\"] == \"sample\"):\n",
    "                loss = self.loss(self.model, self.guide, *args, **kwargs)\n",
    "        # Differentiate the loss.\n",
    "        loss.backward()\n",
    "        # Grab all the parameters from the trace.\n",
    "        params = [site[\"value\"] for site in param_capture.values()]\n",
    "        # Take a step w.r.t. each parameter in params.\n",
    "        self.optim(params)\n",
    "        # Zero out the gradients so that they don't accumulate.\n",
    "        for p in params:\n",
    "            p.grad = p.new_zeros(p.shape)\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# This is a basic implementation of the Evidence Lower Bound, which is the\n",
    "# fundamental objective in Variational Inference.\n",
    "# See http://pyro.ai/examples/svi_part_i.html for details.\n",
    "# This implementation has various limitations (for example it only supports\n",
    "# random variablbes with reparameterized samplers), but all the ELBO\n",
    "# implementations in Pyro share the same basic logic.\n",
    "def elbo(model, guide, *args, **kwargs):\n",
    "    # Run the guide with the arguments passed to SVI.step() and trace the execution,\n",
    "    # i.e. record all the calls to Pyro primitives like sample() and param().\n",
    "    guide_trace = trace(guide).get_trace(*args, **kwargs)\n",
    "    # Now run the model with the same arguments and trace the execution. Because\n",
    "    # model is being run with replay, whenever we encounter a sample site in the\n",
    "    # model, instead of sampling from the corresponding distribution in the model,\n",
    "    # we instead reuse the corresponding sample from the guide. In probabilistic\n",
    "    # terms, this means our loss is constructed as an expectation w.r.t. the joint\n",
    "    # distribution defined by the guide.\n",
    "    model_trace = trace(replay(model, guide_trace)).get_trace(*args, **kwargs)\n",
    "    # We will accumulate the various terms of the ELBO in `elbo`.\n",
    "    elbo = 0.\n",
    "    # Loop over all the sample sites in the model and add the corresponding\n",
    "    # log p(z) term to the ELBO. Note that this will also include any observed\n",
    "    # data, i.e. sample sites with the keyword `obs=...`.\n",
    "    for site in model_trace.values():\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            elbo = elbo + site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
    "    # Loop over all the sample sites in the guide and add the corresponding\n",
    "    # -log q(z) term to the ELBO.\n",
    "    for site in guide_trace.values():\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            elbo = elbo - site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
    "    # Return (-elbo) since by convention we do gradient descent on a loss and\n",
    "    # the ELBO is a lower bound that needs to be maximized.\n",
    "    return -elbo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
