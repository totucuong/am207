{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "We want to maximize the log-likehood of data $\\log p_{\\theta}(x)$ **indirectly** by maximizing its lowwer bound ELBO instead. \n",
    "\n",
    "Recall that \n",
    "$$\\begin{align}ELBO &= E_{q_{\\phi}(z)}\\big[\\log p_{\\theta}(x,z) - \\log(q_{\\phi}(z) \\big]\\\\\n",
    "&= E_{q_{\\phi}(z)}\\log p_{\\theta}(x|z)p(z) - \\log(q_{\\phi}(z) \\big]\\\\\n",
    "&= E_{q_{\\phi}(z)}\\log p_{\\theta}(x|z) + \\log_{\\theta}p(z) - \\log(q_{\\phi}(z) \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "To optimize ELBO we need to be able to compute its gradient wrt to parameters $\\phi, \\theta$. If both model and guide are from exponential family then we can derive this gradient in a closed form. Hence the standard gradient descent can be applied easily. \n",
    "\n",
    "Unfortunately, for a general model and guide, it is not possible to derive a closed form for the gradient. To overcome this, we use Monte Carlo to compute unbiased estimate of the ELBO(q) gradients.\n",
    "\n",
    "$$\\nabla_{\\phi, \\theta}ELBO = \\nabla_{\\phi, \\theta} E_{q_{\\phi}(z)}\\big[\\log p_{\\theta}(x,z) - \\log(q_{\\phi}(z) \\big]$$\n",
    "\n",
    "There are two main questions to ask here:\n",
    "1. How to fastly compute the log likelihood term $\\log p_{\\theta}(x|z)$ since our dataset could contains millions of data points.\n",
    "2. We now use $\\theta$ to denote $\\theta, \\phi$ collectively and ask ourself a more generic quesiton: how do we estimate $\\nabla_\\phi E_{q_\\phi(z)}\\big[f_{\\phi}(z) \\big]?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Log-likelhood evaluation\n",
    "\n",
    "By exploiting the conditional indenpence of data points, we can estimate this term as:\n",
    "\n",
    "$$\\sum_{i=1}^{N}\\log p(x_i | z) \\approx \\frac{N}{M}\\sum_{i \\in I_M}\\log p(x_i|z),$$\n",
    "\n",
    "where $I_M$ is mini-batch indices of dataset.\n",
    "\n",
    "To indicate conditional independence specifically, we can use ``pyro.plate``: \n",
    "\n",
    "```python\n",
    "def model(data):\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data [WE ONLY CHANGE THE NEXT LINE]\n",
    "    for i in pyro.plate(\"data_loop\", len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do it in vectorized fashion:\n",
    "\n",
    "```python\n",
    "with plate('observe_data'):\n",
    "    pyro.sample('obs', dist.Bernoulli(f), obs=data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we have the subsampling for free\n",
    "\n",
    "```python\n",
    "with plate('observe_data', size=10, subsample_size=5) as ind:\n",
    "    pyro.sample('obs', dist.Bernoulli(f),\n",
    "                obs=data.index_select(0, ind))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details can be found [here](http://pyro.ai/examples/svi_part_ii.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO Gradient Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Pyro](http://pyro.ai/examples/svi_part_i.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
