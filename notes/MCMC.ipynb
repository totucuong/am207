{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\Ex}{\\mathbb{E}}\n",
    "\\newcommand{\\Var}{\\mathrm{Var}}\n",
    "\\newcommand{\\Cov}{\\mathrm{Cov}}\n",
    "\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n",
    "\\newcommand{\\indic}{\\mathbb{1}}\n",
    "\\newcommand{\\avg}{\\overline}\n",
    "\\newcommand{\\est}{\\hat}\n",
    "\\newcommand{\\trueval}[1]{#1^{*}}\n",
    "\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\like}{\\cal L}\n",
    "\\renewcommand{\\loglike}{\\ell}\n",
    "\\renewcommand{\\err}{\\cal E}\n",
    "\\renewcommand{\\dat}{\\cal D}\n",
    "\\renewcommand{\\hyp}{\\cal H}\n",
    "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n",
    "\\renewcommand{\\x}{\\mathbf x}\n",
    "\\renewcommand{\\v}[1]{\\mathbf #1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will illustrate how to sample from a Markov chain and introduce two MCMC algorithms: Metropolis-Hastings and Gibbs sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain sampling\n",
    "\n",
    "Suppose we want to simulate the following Markov chain\n",
    "\n",
    "![a markov chain](gfx/amarkovchain.jpg)\n",
    "\n",
    "with transition matrix:\n",
    "\n",
    "![transition matrix](gfx/transm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition matrix\n",
    "Q = np.array([[1/3, 1/3, 1/3, 0], [0,0,1/2,1/2], [0,1,0,0],[1/2,0,0,1/2]])\n",
    "\n",
    "# number of states\n",
    "M = Q.shape[0]\n",
    "\n",
    "# number of simulation\n",
    "nsim = 10**4\n",
    "\n",
    "# storing the result of simulation\n",
    "x = np.zeros(nsim, dtype=int)\n",
    "\n",
    "# seed\n",
    "np.random.seed(2017)\n",
    "\n",
    "# pickuniformly an intial state\n",
    "x[0] = np.random.choice(np.arange(M), size=1)\n",
    "\n",
    "# sampling from the Markov chain\n",
    "for i in range(1,nsim):\n",
    "    x[i] = np.random.choice(np.arange(M), size=1, p=Q[x[i-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we set nsim to a large number, it may be reasonble to believe that the chain is close to stationary during the latter portion of simulation. To check this, we eliminate the first haft of the simulations to give the chain time to reach stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = x[nsim//2:nsim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements, counts = np.unique(samples, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize them we have an approximiation to the stationary distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2088, 0.2924, 0.2234, 0.2754])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts/sum(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the true stationary distribution for this Markov chain is (3/14, 2/7, 3/14, 2/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21428571428571427,\n",
       " 0.2857142857142857,\n",
       " 0.21428571428571427,\n",
       " 0.2857142857142857]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3/14, 2/7, 3/14, 2/7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the our sampling distribution is quite close to the true **stationary distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian statistics, a lot of times we want to peform sampling from a complex distribution which is difficult to deal with. The difficulty arises in evaluating integration to compute the normalization factor. \n",
    "\n",
    "To address this problem, we construct a Markov chain whose stationary distribution is the original distribution that we want to sample from. Since sampling from Markov chain is easy, we can peform sampling from the constructed Markov chain. \n",
    "\n",
    "And because the stationary distribution of the constructed Markov chain is the original distribution, this means if we run the Markov chain long enough, samples from the Markov chain will be samples from the orignal distribution, at least approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Metropolis-Hastings\n",
    "\n",
    "The Metropolis-Hasting algorithm is a general recipe that lets us start with any irreducible Markov chain on the state space of interest and then modify it into a new Markov chain that has desired stationary distribution\n",
    "\n",
    "Our goal is to modify a MC P to construct a MC $X_0, X_1, \\dots$ with stationary distribution $s$. The pseudocode is as follows.\n",
    "\n",
    "1. Start at any state $X_0$ (choosen randomly or deterministically)\n",
    "2. If $X_n = i$, propose a new state j using the transition probabilities in the ith row of the original transition matrix P.\n",
    "3. Compute the acceptance probability $a_{ij} = min(\\frac{s_jp_{ij}}{s_ip_{ij}},1)$\n",
    "4. Flip a coin that lands Heads with probability $a_{ij}$\n",
    "5. If the coin lands Heads, accept the proposal (i.e., go to j) setting $X_{n+1} = j$. Otherwise, reject the proposal (i.e., stay at i), setting $X_{n+1} = i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Metropolis-Hastings algorithm can slo be applied in a continous state space, using pdfs instead of pmfs. This is veery useufl in Bayesian inference, where we oftwent want to study **the posterior distribution** of an unknown parameter. This posterior distribution maybe very complicated to work with analytically, and may have an unknown normalizing constant. \n",
    "\n",
    "The MCMC approach is to obtain a large number of draws fromo a Markov chain whose stationary distribution is the posterior distribution. We can then use these draws to approximate the true posterior distribution. For example, we can estimate the posterior mean using the sample mean of these draws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example: Normal-Normal conjugacy\n",
    "\n",
    "Let $Y \\mid \\theta \\sim \\cal{N}(\\theta, \\sigma^2)$, where $\\sigma^2$ is known but $\\theta$ is unknown.\n",
    "\n",
    "Using Bayesian framework, we treat $\\theta$ as a random variable, with prior given by $\\theta \\sim \\cal{N}(\\mu, \\tau^2)$ for some known constants $\\mu$ and $\\sigma^2$. That is, we have a two level model \n",
    "$$\\theta \\sim \\cal{N}(\\mu, \\tau^2)\\\\ Y \\mid \\theta \\sim \\cal{N}(\\theta, \\sigma^2)$$\n",
    "\n",
    "We will use the Metropolis-Hastings algorithm to find the posterior mean and variance of $\\theta$ after observing the value of $Y$.\n",
    "\n",
    "After observing Y = y, we have $f_{\\theta \\mid y}(\\theta \\mid y) \\propto f_{y\\mid \\theta}(y \\mid \\theta)f_{\\theta}(\\theta) \\propto e^{-\\frac{1}{2\\sigma^2}(y-\\theta)^2}e^{-\\frac{1}{2\\tau^2}(\\theta-\\mu)^2}$. \n",
    "\n",
    "By completing square, we can obtain an explitie formula for the posterior distribution of $\\theta$:\n",
    "\n",
    "![normal normal](gfx/normal_normal.jpg)\n",
    "\n",
    "Suppose that we didn't know how to complete the square (we do not have the above formula). Or we just simply want to verify our formula for a specific values of $y, \\sigma^2, \\mu, \\tau^2$. We can do this by simulating from the posteior distribution of $\\theta$, using the Metropolis-Hastings algorithm to construct a Markov chain whose stationary distribution is $f_{\\theta \\mid y}(\\theta \\mid y)$.\n",
    "\n",
    "A Metropolis-Hastings algorithm for generating $\\theta_0, \\theta_1, \\ldots$ is as follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs sampling\n",
    "\n",
    "Gibss sampling is an MCMC algorithm for obtaining approximate draws from a joint distribution, based on sampling from **conditional** distributions one at a time: at each stage, one variable is updated (keeping all other varibles fixed) by drawing from the condtional distributions of that variable given all other variables.\n",
    "\n",
    "This approach is very useful when we have conditional distributions that are pleasant to work with.\n",
    "\n",
    "We have to major Gibbs samplers:\n",
    "\n",
    "1. Systematic scan Gibbs sampler: components (dimensions, variables) are updated one by one in order\n",
    "2. Random scan Gibbs sampler: a randomly chosen component is update at each stag.\n",
    "\n",
    "We illustrate Gibbs sampler in the context of bivariate $p(X,Y)$ and both $X, Y$ are discrete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Systematic scan Gibbs sampler**: Let X and Y be discrete r.v.s with joint PMF $p_{X,Y}(x,y) = p(X=x, Y=y)$. We wish to construct a two-dimensional Markov chain $(X_n, Y_n)$ whose stationary distribution is $p_{X,Y}$. If the current state is $(X_n, Y_n) = (x_n,y_n)$, then:\n",
    "\n",
    "1. Draw a value $x_{n+1}$ from the conditional distribution of X given $Y = y_n$ and set $X_{n+1} = x_{n+1}$\n",
    "2. Draw a value $y_{n+1}$ from the conditional distribution of Y given $X = x_{n+1}$, and set $Y_{n+1} = y_{n+1}$\n",
    "3. Repeat step 1 and step 2 over and over, the stationary distribution of the chain $(X_0, Y_0),(X_1, Y_1), \\dots$ is $p_{X,Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random scan Gibbs sampler** The difference compared to systematic scan Gibbs sampler is that in each stage, it picks a unifomrly random component and updates it, according to the conditional distributions given the other component:\n",
    "\n",
    "1. Choose which component to update, with equal probabilities.\n",
    "2. If the X-coponent was chosen, draw a value $x_{n+1}$ from the conditional distribution of X given $Y = y_n$ and set $X_{n+1}= x_{n+1}, Y_{n+1} = y_n$. We do similar update if $Y_n$ is choosen to update.\n",
    "3. Repeat steps 1 and 2 over and over, the stationary distribution of the chain $(X_0, Y_0), (X_1, Y_1), \\dots$ is $p_{X,Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
